{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "# import json\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import random\n",
    "# from matplotlib import pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from wordcloud import WordCloud,STOPWORDS\n",
    "# import missingno as msno\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# from keras.preprocessing import text\n",
    "# import keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense,Embedding,LSTM,Dropout\n",
    "# from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# import nltk\n",
    "# from nltk import word_tokenize\n",
    "# from nltk.stem import PorterStemmer\n",
    "\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "# from transformers import pipeline\n",
    "# from transformers import DistilBertTokenizerFast\n",
    "# from transformers import BertForSequenceClassification, BertTokenizerFast\n",
    "# from transformers import TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments\n",
    "# from transformers import BertTokenizer, TFBertForSequenceClassification, BertConfig\n",
    "# from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questionType</th>\n",
       "      <th>askerID</th>\n",
       "      <th>questionTime</th>\n",
       "      <th>questionText</th>\n",
       "      <th>answers</th>\n",
       "      <th>asin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>open-ended</td>\n",
       "      <td>A3VTSR929E3J0R</td>\n",
       "      <td>December 26, 2013</td>\n",
       "      <td>May I request for a manual instruction on this...</td>\n",
       "      <td>[{'answerText': 'Homedics has a complete list ...</td>\n",
       "      <td>B000050FES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>open-ended</td>\n",
       "      <td>A1LZ5DJ7AJYAXD</td>\n",
       "      <td>April 26, 2014</td>\n",
       "      <td>Having problems with the heater melting the wa...</td>\n",
       "      <td>[{'answerText': 'I leave the unit on all the t...</td>\n",
       "      <td>B000050FES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yes/no</td>\n",
       "      <td>A1ER8W5FAQM6G5</td>\n",
       "      <td>July 11, 2014</td>\n",
       "      <td>How much wax (in pounds, for instance) does it...</td>\n",
       "      <td>[{'answerText': 'It came with the proper amoun...</td>\n",
       "      <td>B000050FES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>open-ended</td>\n",
       "      <td>A1P403R4I7LVPU</td>\n",
       "      <td>December 20, 2013</td>\n",
       "      <td>How to know the expired date of this product?</td>\n",
       "      <td>[{'answerText': 'The expiration date is on the...</td>\n",
       "      <td>B000052YQ2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>open-ended</td>\n",
       "      <td>A166KM8A5H1Z2O</td>\n",
       "      <td>February 26, 2014</td>\n",
       "      <td>I am not in the sun as I work inside. I am new...</td>\n",
       "      <td>[{'answerText': 'This product is awesome. I do...</td>\n",
       "      <td>B000052YQ2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32931</th>\n",
       "      <td>yes/no</td>\n",
       "      <td>A164SPHD6JR4L0</td>\n",
       "      <td>July 2, 2014</td>\n",
       "      <td>is Argan Oil Pure 100% good for Skin?</td>\n",
       "      <td>[{'answerText': 'I really like it.  It is a no...</td>\n",
       "      <td>B00L5JHZJO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32932</th>\n",
       "      <td>yes/no</td>\n",
       "      <td>A3LMNODD16O8J6</td>\n",
       "      <td>July 2, 2014</td>\n",
       "      <td>I find myself with rough cuticles right around...</td>\n",
       "      <td>[{'answerText': 'Yes, you can. In the evening ...</td>\n",
       "      <td>B00L5JHZJO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32933</th>\n",
       "      <td>yes/no</td>\n",
       "      <td>A13S7ZT69W96HR</td>\n",
       "      <td>July 2, 2014</td>\n",
       "      <td>is it good for nail beauty?</td>\n",
       "      <td>[{'answerText': 'I would say it's good for cut...</td>\n",
       "      <td>B00L5JHZJO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32934</th>\n",
       "      <td>open-ended</td>\n",
       "      <td>A1HHOCBWVDXSE</td>\n",
       "      <td>July 2, 2014</td>\n",
       "      <td>how can i use it for Topical Use on Dry Hair?</td>\n",
       "      <td>[{'answerText': 'A little goes a long way!  A ...</td>\n",
       "      <td>B00L5JHZJO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32935</th>\n",
       "      <td>open-ended</td>\n",
       "      <td>A1ZJ90GJNNH2EK</td>\n",
       "      <td>July 2, 2014</td>\n",
       "      <td>how can i use it for Deep Conditioning Session?</td>\n",
       "      <td>[{'answerText': 'You can use it as a pre-shamp...</td>\n",
       "      <td>B00L5JHZJO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32936 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      questionType         askerID       questionTime  \\\n",
       "0       open-ended  A3VTSR929E3J0R  December 26, 2013   \n",
       "1       open-ended  A1LZ5DJ7AJYAXD     April 26, 2014   \n",
       "2           yes/no  A1ER8W5FAQM6G5      July 11, 2014   \n",
       "3       open-ended  A1P403R4I7LVPU  December 20, 2013   \n",
       "4       open-ended  A166KM8A5H1Z2O  February 26, 2014   \n",
       "...            ...             ...                ...   \n",
       "32931       yes/no  A164SPHD6JR4L0       July 2, 2014   \n",
       "32932       yes/no  A3LMNODD16O8J6       July 2, 2014   \n",
       "32933       yes/no  A13S7ZT69W96HR       July 2, 2014   \n",
       "32934   open-ended   A1HHOCBWVDXSE       July 2, 2014   \n",
       "32935   open-ended  A1ZJ90GJNNH2EK       July 2, 2014   \n",
       "\n",
       "                                            questionText  \\\n",
       "0      May I request for a manual instruction on this...   \n",
       "1      Having problems with the heater melting the wa...   \n",
       "2      How much wax (in pounds, for instance) does it...   \n",
       "3          How to know the expired date of this product?   \n",
       "4      I am not in the sun as I work inside. I am new...   \n",
       "...                                                  ...   \n",
       "32931              is Argan Oil Pure 100% good for Skin?   \n",
       "32932  I find myself with rough cuticles right around...   \n",
       "32933                        is it good for nail beauty?   \n",
       "32934      how can i use it for Topical Use on Dry Hair?   \n",
       "32935    how can i use it for Deep Conditioning Session?   \n",
       "\n",
       "                                                 answers        asin  \n",
       "0      [{'answerText': 'Homedics has a complete list ...  B000050FES  \n",
       "1      [{'answerText': 'I leave the unit on all the t...  B000050FES  \n",
       "2      [{'answerText': 'It came with the proper amoun...  B000050FES  \n",
       "3      [{'answerText': 'The expiration date is on the...  B000052YQ2  \n",
       "4      [{'answerText': 'This product is awesome. I do...  B000052YQ2  \n",
       "...                                                  ...         ...  \n",
       "32931  [{'answerText': 'I really like it.  It is a no...  B00L5JHZJO  \n",
       "32932  [{'answerText': 'Yes, you can. In the evening ...  B00L5JHZJO  \n",
       "32933  [{'answerText': 'I would say it's good for cut...  B00L5JHZJO  \n",
       "32934  [{'answerText': 'A little goes a long way!  A ...  B00L5JHZJO  \n",
       "32935  [{'answerText': 'You can use it as a pre-shamp...  B00L5JHZJO  \n",
       "\n",
       "[32936 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "    data = list(parse(path))\n",
    "    df = pd.json_normalize(data, 'questions', ['asin'])\n",
    "    return df\n",
    "\n",
    "# Replace 'QA_Beauty.json.gz' with the actual path to your Gzip-compressed JSON file\n",
    "df = getDF('QA_Beauty.json.gz')\n",
    "\n",
    "# Display the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "questionType                                           open-ended\n",
      "askerID                                            A3VTSR929E3J0R\n",
      "questionTime                                    December 26, 2013\n",
      "questionText    May I request for a manual instruction on this...\n",
      "answers         [{'answerText': 'Homedics has a complete list ...\n",
      "asin                                                   B000050FES\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "first_row = df.iloc[0]\n",
    "print(first_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of question type\n",
    "df['questionType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "sns.countplot(x='questionType', data=df)\n",
    "plt.title('Count Plot of Question Types')\n",
    "plt.xlabel('Question Type')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['asin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = df['asin'].unique()\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Word cloud for 'questionText'\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text = ' '.join(df['questionText'])\n",
    "wordcloud = WordCloud(width=800, height=400, random_state=42).generate(text)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find distribution of askerID to see the frequent question askers\n",
    "df['askerID'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame with the 'questionTime' column\n",
    "df['questionTime'] = pd.to_datetime(df['questionTime'], format='%B %d, %Y', errors='coerce')\n",
    "\n",
    "# Drop rows with NaT (Not a Time) values after conversion\n",
    "df = df.dropna(subset=['questionTime'])\n",
    "\n",
    "# Extract relevant temporal features\n",
    "df['year'] = df['questionTime'].dt.year\n",
    "df['month'] = df['questionTime'].dt.month\n",
    "df['day_of_week'] = df['questionTime'].dt.day_of_week\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming df is your DataFrame with the 'questionTime' column already converted to datetime\n",
    "# If not, follow the previous response to convert it.\n",
    "\n",
    "# Set the 'questionTime' column as the index\n",
    "df.set_index('questionTime', inplace=True)\n",
    "\n",
    "# Resample the data to a monthly frequency\n",
    "monthly_counts = df.resample('M').size()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x=monthly_counts.index, y=monthly_counts.values)\n",
    "plt.title('Monthly Trend of Questions')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Number of Questions')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the sample of questions\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# If 'questionText' contains strings, you can calculate the length\n",
    "df['questionText_length'] = df['questionText'].apply(len)\n",
    "\n",
    "# Display the length and content of questions\n",
    "print(\"Length of Questions:\")\n",
    "print(df[['questionText', 'questionText_length']].head())\n",
    "\n",
    "# Explore the content of questions\n",
    "print(\"\\nSample Questions:\")\n",
    "for index, question in enumerate(df['questionText'].head()):\n",
    "    print(f\"{index + 1}. {question}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "\n",
    "# Count the number of questions and answers for each product ('asin')\n",
    "product_distribution = df.groupby('asin').agg({'questionText': 'size', 'answers': lambda x: x.apply(len).sum()})\n",
    "\n",
    "# Rename the columns for clarity\n",
    "product_distribution = product_distribution.rename(columns={'questionText': 'question_count', 'answers': 'answer_count'})\n",
    "\n",
    "# Display the distribution DataFrame\n",
    "print(\"Distribution of Questions and Answers for Each Product:\")\n",
    "print(product_distribution)\n",
    "\n",
    "# Visualize the distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=product_distribution.index, y='question_count', data=product_distribution, color='skyblue', label='Questions')\n",
    "sns.barplot(x=product_distribution.index, y='answer_count', data=product_distribution, color='orange', label='Answers')\n",
    "plt.xlabel('Product (asin)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Questions and Answers for Each Product')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "\n",
    "# Count the number of questions and answers for each product ('asin')\n",
    "product_distribution = df.groupby('asin').agg({'questionText': 'size', 'answers': lambda x: x.apply(len).sum()})\n",
    "\n",
    "# Rename the columns for clarity\n",
    "product_distribution = product_distribution.rename(columns={'questionText': 'question_count', 'answers': 'answer_count'})\n",
    "\n",
    "# Identify popular products based on questions\n",
    "popular_products_questions = product_distribution.sort_values(by='question_count', ascending=False)\n",
    "\n",
    "# Identify popular products based on answers\n",
    "popular_products_answers = product_distribution.sort_values(by='answer_count', ascending=False)\n",
    "\n",
    "# Display the popular products based on questions and answers\n",
    "print(\"Popular Products Based on Questions:\")\n",
    "print(popular_products_questions)\n",
    "\n",
    "print(\"\\nPopular Products Based on Answers:\")\n",
    "print(popular_products_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your existing DataFrame is named 'original_df'\n",
    "#original_df = ...\n",
    "\n",
    "# Create a new DataFrame with only 'questionText','answers' and 'questionType'\n",
    "new_df = df[['questionText', 'answers','questionType']].copy()\n",
    "\n",
    "# Display the new DataFrame\n",
    "print(new_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named 'original_df'\n",
    "#original_df = ...\n",
    "\n",
    "# Create a new DataFrame with only 'questionText' and 'answers'\n",
    "new_df = df[['questionType','questionText', 'answers']].copy()\n",
    "\n",
    "# Reset the index if needed\n",
    "new_df = new_df.reset_index()\n",
    "\n",
    "# Display the new DataFrame\n",
    "print(new_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = new_df.drop(columns=['questionTime'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame with the 'answers' column\n",
    "df1['answerText'] = df1['answers'].apply(lambda x: x[0]['answerText'] if x else None)\n",
    "\n",
    "# Drop the 'answers' column\n",
    "df1 = df1.drop(columns=['answers'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['questionText'].loc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['answerText'].loc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming 'answers' is the column containing lists of answers\n",
    "df['num_answers'] = df['answers'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "\n",
    "# Plot histogram\n",
    "sns.histplot(df['num_answers'], bins=20, kde=True)\n",
    "plt.title('Distribution of Number of Answers for Each Question')\n",
    "plt.xlabel('Number of Answers')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'answers' is the column containing lists of answers\n",
    "df['num_answers'] = df['answers'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "\n",
    "# Count occurrences of different numbers of answers\n",
    "answer_counts = df['num_answers'].value_counts()\n",
    "\n",
    "# Display the result\n",
    "print(answer_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df1 is your DataFrame with columns 'questionType', 'questionText', and 'answerText'\n",
    "# Replace this with your actual DataFrame\n",
    "# df1 = ...\n",
    "\n",
    "# Combine question and answer text for processing\n",
    "texts = df1['questionText'] + ' ' + df1['answerText']\n",
    "\n",
    "# Tokenize and lemmatize the words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenized_texts = [word_tokenize(text.lower()) for text in texts]\n",
    "lemmatized_texts = [[lemmatizer.lemmatize(word) for word in tokens] for tokens in tokenized_texts]\n",
    "\n",
    "# Convert the lemmatized texts back to strings\n",
    "preprocessed_texts = [' '.join(tokens) for tokens in lemmatized_texts]\n",
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(preprocessed_texts)\n",
    "\n",
    "# Create a list of question types\n",
    "question_types = df1['questionType'].unique()\n",
    "\n",
    "# Create training data for each question type\n",
    "training_data = []\n",
    "for question_type in question_types:\n",
    "    subset_df = df1[df1['questionType'] == question_type]\n",
    "    \n",
    "    # Create bag of words representation for each question\n",
    "    for index, row in subset_df.iterrows():\n",
    "        bag = X_tfidf[index].toarray().flatten().tolist()\n",
    "        output_row = [0] * len(question_types)\n",
    "        output_row[np.where(question_types == question_type)[0][0]] = 1\n",
    "        training_data.append([bag, output_row])\n",
    "\n",
    "# Shuffle training data and convert to NumPy array\n",
    "random.shuffle(training_data)\n",
    "training_data = np.array(training_data)\n",
    "\n",
    "# Separate features (X) and labels (Y)\n",
    "train_x = list(training_data[:, 0])\n",
    "train_y = list(training_data[:, 1])\n",
    "\n",
    "print(\"Training data created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our training data\n",
    "training = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)\n",
    "# training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words\n",
    "    pattern_words = doc[0]\n",
    "    # convert pattern_words in lower case\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "    # create bag of words array,if word match found in current pattern then put 1 otherwise 0.[row * colm(263)]\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "    \n",
    "    # in output array 0 value for each tag ang 1 value for matched tag.[row * colm(8)]\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    \n",
    "    training.append([bag, output_row])\n",
    "# shuffle training and turn into np.array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "# create train and test. X - patterns(words), Y - intents(tags)\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])\n",
    "print(\"Training data created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the necessary NLTK data\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "ignore_words=['?', '!', ',', '.']\n",
    "\n",
    "def preprocess_pattern(pattern):\n",
    "    words = word_tokenize(pattern.lower())\n",
    "    stemmed_words = [stemmer.stem(word) for word in words if word not in ignore_words]\n",
    "    return \" \".join(stemmed_words)  \n",
    "\n",
    "df['questionText'] = df['questionText'].apply(preprocess_pattern)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['questionText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "ignore_words=['?', '!', ',', '.']\n",
    "\n",
    "def preprocess_pattern(pattern):\n",
    "    words = word_tokenize(pattern.lower())\n",
    "    stemmed_words = [stemmer.stem(word) for word in words if word not in ignore_words]\n",
    "    return \" \".join(stemmed_words)  \n",
    "\n",
    "df1['answerText'] = df1['answerText'].apply(preprocess_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['answerText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize, lower each word and remove duplicates\n",
    "import pickle\n",
    "words=[]\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?', '!']\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "# sort classes\n",
    "classes = sorted(list(set(classes)))\n",
    "# documents = combination between patterns and intents\n",
    "print (len(documents), \"documents\\n\", documents, \"\\n\")\n",
    "# classes = intents[tag]\n",
    "print (len(classes), \"classes\\n\", classes, \"\\n\")\n",
    "# words = all words, vocabulary\n",
    "print (len(words), \"unique lemmatized words\\n\", words, \"\\n\")\n",
    "pickle.dump(words,open('words.pkl','wb'))\n",
    "pickle.dump(classes,open('classes.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df1['combined_text'] = df1['questionText'] + ' ' + df1['answerText']\n",
    "\n",
    "# Lowercasing\n",
    "df1['combined_text'] = df1['combined_text'].str.lower()\n",
    "\n",
    "# Tokenization and removing punctuation\n",
    "df1['combined_text'] = df1['combined_text'].apply(lambda x: word_tokenize(x.translate(str.maketrans('', '', string.punctuation))))\n",
    "\n",
    "# Removing stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df1['combined_text'] = df1['combined_text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df1['combined_text'] = df1['combined_text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "# Joining the tokens back into sentences\n",
    "df1['processed_text'] = df1['combined_text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Drop intermediate columns\n",
    "df1 = df1.drop(['combined_text'], axis=1)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df1.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Sample data\n",
    "# data = [\n",
    "#     (\"May I request for a manual instruction on this...\", \"Instruction\"),\n",
    "#     (\"Having problems with the heater melting the wax...\", \"Heater Issue\"),\n",
    "#     (\"How much wax in pounds for instance does it take?\", \"Wax Quantity\"),\n",
    "#     (\"How to know the expired date of this product?\", \"Expiration Date\"),\n",
    "#     (\"I am not in the sun as I work inside...\", \"Sun Exposure\"),\n",
    "#     # Add more examples with corresponding intents\n",
    "# ]\n",
    "data = df1\n",
    "\n",
    "# Separate data into features (X) and labels (Y)\n",
    "X, Y = zip(*data)\n",
    "\n",
    "# Vectorize the text data\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Multinomial Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Example prediction for a new input\n",
    "new_input = [\"I have a question about product usage.\"]\n",
    "new_input_tfidf = tfidf_vectorizer.transform(new_input)\n",
    "predicted_intent = clf.predict(new_input_tfidf)\n",
    "print(\"Predicted Intent:\", predicted_intent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
