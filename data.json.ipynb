{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questionType</th>\n",
       "      <th>askerID</th>\n",
       "      <th>questionTime</th>\n",
       "      <th>questionText</th>\n",
       "      <th>answers</th>\n",
       "      <th>asin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>open-ended</td>\n",
       "      <td>A3VTSR929E3J0R</td>\n",
       "      <td>December 26, 2013</td>\n",
       "      <td>May I request for a manual instruction on this...</td>\n",
       "      <td>[{'answerText': 'Homedics has a complete list ...</td>\n",
       "      <td>B000050FES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>open-ended</td>\n",
       "      <td>A1LZ5DJ7AJYAXD</td>\n",
       "      <td>April 26, 2014</td>\n",
       "      <td>Having problems with the heater melting the wa...</td>\n",
       "      <td>[{'answerText': 'I leave the unit on all the t...</td>\n",
       "      <td>B000050FES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yes/no</td>\n",
       "      <td>A1ER8W5FAQM6G5</td>\n",
       "      <td>July 11, 2014</td>\n",
       "      <td>How much wax (in pounds, for instance) does it...</td>\n",
       "      <td>[{'answerText': 'It came with the proper amoun...</td>\n",
       "      <td>B000050FES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>open-ended</td>\n",
       "      <td>A1P403R4I7LVPU</td>\n",
       "      <td>December 20, 2013</td>\n",
       "      <td>How to know the expired date of this product?</td>\n",
       "      <td>[{'answerText': 'The expiration date is on the...</td>\n",
       "      <td>B000052YQ2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>open-ended</td>\n",
       "      <td>A166KM8A5H1Z2O</td>\n",
       "      <td>February 26, 2014</td>\n",
       "      <td>I am not in the sun as I work inside. I am new...</td>\n",
       "      <td>[{'answerText': 'This product is awesome. I do...</td>\n",
       "      <td>B000052YQ2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32931</th>\n",
       "      <td>yes/no</td>\n",
       "      <td>A164SPHD6JR4L0</td>\n",
       "      <td>July 2, 2014</td>\n",
       "      <td>is Argan Oil Pure 100% good for Skin?</td>\n",
       "      <td>[{'answerText': 'I really like it.  It is a no...</td>\n",
       "      <td>B00L5JHZJO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32932</th>\n",
       "      <td>yes/no</td>\n",
       "      <td>A3LMNODD16O8J6</td>\n",
       "      <td>July 2, 2014</td>\n",
       "      <td>I find myself with rough cuticles right around...</td>\n",
       "      <td>[{'answerText': 'Yes, you can. In the evening ...</td>\n",
       "      <td>B00L5JHZJO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32933</th>\n",
       "      <td>yes/no</td>\n",
       "      <td>A13S7ZT69W96HR</td>\n",
       "      <td>July 2, 2014</td>\n",
       "      <td>is it good for nail beauty?</td>\n",
       "      <td>[{'answerText': 'I would say it's good for cut...</td>\n",
       "      <td>B00L5JHZJO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32934</th>\n",
       "      <td>open-ended</td>\n",
       "      <td>A1HHOCBWVDXSE</td>\n",
       "      <td>July 2, 2014</td>\n",
       "      <td>how can i use it for Topical Use on Dry Hair?</td>\n",
       "      <td>[{'answerText': 'A little goes a long way!  A ...</td>\n",
       "      <td>B00L5JHZJO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32935</th>\n",
       "      <td>open-ended</td>\n",
       "      <td>A1ZJ90GJNNH2EK</td>\n",
       "      <td>July 2, 2014</td>\n",
       "      <td>how can i use it for Deep Conditioning Session?</td>\n",
       "      <td>[{'answerText': 'You can use it as a pre-shamp...</td>\n",
       "      <td>B00L5JHZJO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32936 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      questionType         askerID       questionTime  \\\n",
       "0       open-ended  A3VTSR929E3J0R  December 26, 2013   \n",
       "1       open-ended  A1LZ5DJ7AJYAXD     April 26, 2014   \n",
       "2           yes/no  A1ER8W5FAQM6G5      July 11, 2014   \n",
       "3       open-ended  A1P403R4I7LVPU  December 20, 2013   \n",
       "4       open-ended  A166KM8A5H1Z2O  February 26, 2014   \n",
       "...            ...             ...                ...   \n",
       "32931       yes/no  A164SPHD6JR4L0       July 2, 2014   \n",
       "32932       yes/no  A3LMNODD16O8J6       July 2, 2014   \n",
       "32933       yes/no  A13S7ZT69W96HR       July 2, 2014   \n",
       "32934   open-ended   A1HHOCBWVDXSE       July 2, 2014   \n",
       "32935   open-ended  A1ZJ90GJNNH2EK       July 2, 2014   \n",
       "\n",
       "                                            questionText  \\\n",
       "0      May I request for a manual instruction on this...   \n",
       "1      Having problems with the heater melting the wa...   \n",
       "2      How much wax (in pounds, for instance) does it...   \n",
       "3          How to know the expired date of this product?   \n",
       "4      I am not in the sun as I work inside. I am new...   \n",
       "...                                                  ...   \n",
       "32931              is Argan Oil Pure 100% good for Skin?   \n",
       "32932  I find myself with rough cuticles right around...   \n",
       "32933                        is it good for nail beauty?   \n",
       "32934      how can i use it for Topical Use on Dry Hair?   \n",
       "32935    how can i use it for Deep Conditioning Session?   \n",
       "\n",
       "                                                 answers        asin  \n",
       "0      [{'answerText': 'Homedics has a complete list ...  B000050FES  \n",
       "1      [{'answerText': 'I leave the unit on all the t...  B000050FES  \n",
       "2      [{'answerText': 'It came with the proper amoun...  B000050FES  \n",
       "3      [{'answerText': 'The expiration date is on the...  B000052YQ2  \n",
       "4      [{'answerText': 'This product is awesome. I do...  B000052YQ2  \n",
       "...                                                  ...         ...  \n",
       "32931  [{'answerText': 'I really like it.  It is a no...  B00L5JHZJO  \n",
       "32932  [{'answerText': 'Yes, you can. In the evening ...  B00L5JHZJO  \n",
       "32933  [{'answerText': 'I would say it's good for cut...  B00L5JHZJO  \n",
       "32934  [{'answerText': 'A little goes a long way!  A ...  B00L5JHZJO  \n",
       "32935  [{'answerText': 'You can use it as a pre-shamp...  B00L5JHZJO  \n",
       "\n",
       "[32936 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "    data = list(parse(path))\n",
    "    df = pd.json_normalize(data, 'questions', ['asin'])\n",
    "    return df\n",
    "\n",
    "# Replace 'QA_Beauty.json.gz' with the actual path to your Gzip-compressed JSON file\n",
    "df = getDF('QA_Beauty.json.gz')\n",
    "\n",
    "# Display the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            questionText  \\\n",
      "0      May I request for a manual instruction on this...   \n",
      "1      Having problems with the heater melting the wa...   \n",
      "2      How much wax (in pounds, for instance) does it...   \n",
      "3          How to know the expired date of this product?   \n",
      "4      I am not in the sun as I work inside. I am new...   \n",
      "...                                                  ...   \n",
      "32931              is Argan Oil Pure 100% good for Skin?   \n",
      "32932  I find myself with rough cuticles right around...   \n",
      "32933                        is it good for nail beauty?   \n",
      "32934      how can i use it for Topical Use on Dry Hair?   \n",
      "32935    how can i use it for Deep Conditioning Session?   \n",
      "\n",
      "                                              top_answer  \n",
      "0                                                     []  \n",
      "1                                                     []  \n",
      "2                                                     []  \n",
      "3                                                     []  \n",
      "4                                                     []  \n",
      "...                                                  ...  \n",
      "32931  [{'answerText': 'Yes.... it is high in vitamin...  \n",
      "32932  [{'answerText': 'Yes, you can. In the evening ...  \n",
      "32933                                                 []  \n",
      "32934                                                 []  \n",
      "32935                                                 []  \n",
      "\n",
      "[32936 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'answerScore' is the column representing the answer score\n",
    "threshold_score = 0.8\n",
    "\n",
    "# Define a function to filter answers based on the threshold score\n",
    "def filter_answers(answers):\n",
    "    return [answer for answer in answers if float(answer.get('answerScore', 0)) > threshold_score]\n",
    "\n",
    "# Apply the filtering function to the 'answers' column\n",
    "df['top_answer'] = df['answers'].apply(filter_answers)\n",
    "\n",
    "# Print the resulting DataFrame with filtered answers\n",
    "print(df[['questionText', 'top_answer']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questionType</th>\n",
       "      <th>askerID</th>\n",
       "      <th>questionTime</th>\n",
       "      <th>questionText</th>\n",
       "      <th>answers</th>\n",
       "      <th>asin</th>\n",
       "      <th>top_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>open-ended</td>\n",
       "      <td>A3VTSR929E3J0R</td>\n",
       "      <td>December 26, 2013</td>\n",
       "      <td>May I request for a manual instruction on this...</td>\n",
       "      <td>[{'answerText': 'Homedics has a complete list ...</td>\n",
       "      <td>B000050FES</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>open-ended</td>\n",
       "      <td>A1LZ5DJ7AJYAXD</td>\n",
       "      <td>April 26, 2014</td>\n",
       "      <td>Having problems with the heater melting the wa...</td>\n",
       "      <td>[{'answerText': 'I leave the unit on all the t...</td>\n",
       "      <td>B000050FES</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yes/no</td>\n",
       "      <td>A1ER8W5FAQM6G5</td>\n",
       "      <td>July 11, 2014</td>\n",
       "      <td>How much wax (in pounds, for instance) does it...</td>\n",
       "      <td>[{'answerText': 'It came with the proper amoun...</td>\n",
       "      <td>B000050FES</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>open-ended</td>\n",
       "      <td>A1P403R4I7LVPU</td>\n",
       "      <td>December 20, 2013</td>\n",
       "      <td>How to know the expired date of this product?</td>\n",
       "      <td>[{'answerText': 'The expiration date is on the...</td>\n",
       "      <td>B000052YQ2</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>open-ended</td>\n",
       "      <td>A166KM8A5H1Z2O</td>\n",
       "      <td>February 26, 2014</td>\n",
       "      <td>I am not in the sun as I work inside. I am new...</td>\n",
       "      <td>[{'answerText': 'This product is awesome. I do...</td>\n",
       "      <td>B000052YQ2</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32931</th>\n",
       "      <td>yes/no</td>\n",
       "      <td>A164SPHD6JR4L0</td>\n",
       "      <td>July 2, 2014</td>\n",
       "      <td>is Argan Oil Pure 100% good for Skin?</td>\n",
       "      <td>[{'answerText': 'I really like it.  It is a no...</td>\n",
       "      <td>B00L5JHZJO</td>\n",
       "      <td>[{'answerText': 'Yes.... it is high in vitamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32932</th>\n",
       "      <td>yes/no</td>\n",
       "      <td>A3LMNODD16O8J6</td>\n",
       "      <td>July 2, 2014</td>\n",
       "      <td>I find myself with rough cuticles right around...</td>\n",
       "      <td>[{'answerText': 'Yes, you can. In the evening ...</td>\n",
       "      <td>B00L5JHZJO</td>\n",
       "      <td>[{'answerText': 'Yes, you can. In the evening ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32933</th>\n",
       "      <td>yes/no</td>\n",
       "      <td>A13S7ZT69W96HR</td>\n",
       "      <td>July 2, 2014</td>\n",
       "      <td>is it good for nail beauty?</td>\n",
       "      <td>[{'answerText': 'I would say it's good for cut...</td>\n",
       "      <td>B00L5JHZJO</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32934</th>\n",
       "      <td>open-ended</td>\n",
       "      <td>A1HHOCBWVDXSE</td>\n",
       "      <td>July 2, 2014</td>\n",
       "      <td>how can i use it for Topical Use on Dry Hair?</td>\n",
       "      <td>[{'answerText': 'A little goes a long way!  A ...</td>\n",
       "      <td>B00L5JHZJO</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32935</th>\n",
       "      <td>open-ended</td>\n",
       "      <td>A1ZJ90GJNNH2EK</td>\n",
       "      <td>July 2, 2014</td>\n",
       "      <td>how can i use it for Deep Conditioning Session?</td>\n",
       "      <td>[{'answerText': 'You can use it as a pre-shamp...</td>\n",
       "      <td>B00L5JHZJO</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32936 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      questionType         askerID       questionTime  \\\n",
       "0       open-ended  A3VTSR929E3J0R  December 26, 2013   \n",
       "1       open-ended  A1LZ5DJ7AJYAXD     April 26, 2014   \n",
       "2           yes/no  A1ER8W5FAQM6G5      July 11, 2014   \n",
       "3       open-ended  A1P403R4I7LVPU  December 20, 2013   \n",
       "4       open-ended  A166KM8A5H1Z2O  February 26, 2014   \n",
       "...            ...             ...                ...   \n",
       "32931       yes/no  A164SPHD6JR4L0       July 2, 2014   \n",
       "32932       yes/no  A3LMNODD16O8J6       July 2, 2014   \n",
       "32933       yes/no  A13S7ZT69W96HR       July 2, 2014   \n",
       "32934   open-ended   A1HHOCBWVDXSE       July 2, 2014   \n",
       "32935   open-ended  A1ZJ90GJNNH2EK       July 2, 2014   \n",
       "\n",
       "                                            questionText  \\\n",
       "0      May I request for a manual instruction on this...   \n",
       "1      Having problems with the heater melting the wa...   \n",
       "2      How much wax (in pounds, for instance) does it...   \n",
       "3          How to know the expired date of this product?   \n",
       "4      I am not in the sun as I work inside. I am new...   \n",
       "...                                                  ...   \n",
       "32931              is Argan Oil Pure 100% good for Skin?   \n",
       "32932  I find myself with rough cuticles right around...   \n",
       "32933                        is it good for nail beauty?   \n",
       "32934      how can i use it for Topical Use on Dry Hair?   \n",
       "32935    how can i use it for Deep Conditioning Session?   \n",
       "\n",
       "                                                 answers        asin  \\\n",
       "0      [{'answerText': 'Homedics has a complete list ...  B000050FES   \n",
       "1      [{'answerText': 'I leave the unit on all the t...  B000050FES   \n",
       "2      [{'answerText': 'It came with the proper amoun...  B000050FES   \n",
       "3      [{'answerText': 'The expiration date is on the...  B000052YQ2   \n",
       "4      [{'answerText': 'This product is awesome. I do...  B000052YQ2   \n",
       "...                                                  ...         ...   \n",
       "32931  [{'answerText': 'I really like it.  It is a no...  B00L5JHZJO   \n",
       "32932  [{'answerText': 'Yes, you can. In the evening ...  B00L5JHZJO   \n",
       "32933  [{'answerText': 'I would say it's good for cut...  B00L5JHZJO   \n",
       "32934  [{'answerText': 'A little goes a long way!  A ...  B00L5JHZJO   \n",
       "32935  [{'answerText': 'You can use it as a pre-shamp...  B00L5JHZJO   \n",
       "\n",
       "                                              top_answer  \n",
       "0                                                     []  \n",
       "1                                                     []  \n",
       "2                                                     []  \n",
       "3                                                     []  \n",
       "4                                                     []  \n",
       "...                                                  ...  \n",
       "32931  [{'answerText': 'Yes.... it is high in vitamin...  \n",
       "32932  [{'answerText': 'Yes, you can. In the evening ...  \n",
       "32933                                                 []  \n",
       "32934                                                 []  \n",
       "32935                                                 []  \n",
       "\n",
       "[32936 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data has been written to 'intent_data.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Aggregate questions and answers into a list of dictionaries under a generic tag\n",
    "intent_data = {\n",
    "    \"tag\": \"General Inquiry\",\n",
    "    \"questions\": df['questionText'].tolist(),\n",
    "    \"responses\": df['top_answer'].tolist()\n",
    "}\n",
    "\n",
    "# Wrap the single tag data in a list to match the desired format\n",
    "intent_json = [intent_data]\n",
    "\n",
    "# Convert to JSON string\n",
    "intent_json_str = json.dumps(intent_json, indent=4)\n",
    "\n",
    "# Output the JSON string to a file\n",
    "with open('intent_data.json', 'w') as json_file:\n",
    "    json_file.write(intent_json_str)\n",
    "\n",
    "print(\"JSON data has been written to 'intent_data.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag: General Inquiry\n",
      "Number of Questions: 32936\n",
      "Number of Responses: 32936\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON data from the file\n",
    "with open('intent_data.json', 'r') as json_file:\n",
    "    loaded_intent_data = json.load(json_file)\n",
    "\n",
    "# Access the loaded data\n",
    "tag = loaded_intent_data[0][\"tag\"]\n",
    "questions = loaded_intent_data[0][\"questions\"]\n",
    "responses = loaded_intent_data[0][\"responses\"]\n",
    "\n",
    "# Now you can use 'tag', 'questions', and 'responses' as needed\n",
    "print(\"Tag:\", tag)\n",
    "print(\"Number of Questions:\", len(questions))\n",
    "print(\"Number of Responses:\", len(responses))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?', '!']\n",
    "\n",
    "# Load the intent data from the JSON file\n",
    "with open('intent_data.json', 'r') as json_file:\n",
    "    intents = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 documents\n",
      " [] \n",
      "\n",
      "0 classes\n",
      " [] \n",
      "\n",
      "0 unique lemmatized words\n",
      " [] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  \n",
    "\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?', '!']\n",
    "# Load the intent data from the JSON file\n",
    "with open('intent_data.json', 'r') as json_file:\n",
    "    intents = json.load(json_file)\n",
    "for intent in intents:\n",
    "    # Assuming each intent has a key 'patterns', modify this according to your data structure\n",
    "    patterns = intent.get('patterns', [])\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        # Tokenize each word\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        words.extend(w)  # Add each element into the list 'words'\n",
    "        # Combination between patterns and intents\n",
    "        documents.append((w, intent['tag']))  # Add single element to the end of the list 'documents'\n",
    "        # Add tag to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])  # Add tag to the list 'classes'\n",
    "\n",
    "# Lemmatize, lower each word, and remove duplicates\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "# Sort classes\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "# Display the information\n",
    "print(len(documents), \"documents\\n\", documents, \"\\n\")\n",
    "print(len(classes), \"classes\\n\", classes, \"\\n\")\n",
    "print(len(words), \"unique lemmatized words\\n\", words, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output information has been written to 'output_info.txt'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load the intent data from the JSON file\n",
    "with open('intent_data.json', 'r') as json_file:\n",
    "    intents = json.load(json_file)\n",
    "\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?', '!']\n",
    "\n",
    "for intent in intents:\n",
    "    # Assuming each intent has a key 'patterns', modify this according to your data structure\n",
    "    patterns = intent.get('patterns', [])\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        # Tokenize each word\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        words.extend(w)  # Add each element into the list 'words'\n",
    "        # Combination between patterns and intents\n",
    "        documents.append((w, intent['tag']))  # Add a single element to the end of the list 'documents'\n",
    "        # Add tag to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])  # Add tag to the list 'classes'\n",
    "\n",
    "# Lemmatize, lowercase each word, and remove duplicates\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "# Sort classes\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "output_file_path = 'output_info.txt'\n",
    "\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    output_file.write(f\"{len(documents)} documents\\n{documents}\\n\\n\")\n",
    "    output_file.write(f\"{len(classes)} classes\\n{classes}\\n\\n\")\n",
    "    output_file.write(f\"{len(words)} unique lemmatized words\\n{words}\\n\\n\")\n",
    "\n",
    "print(\"Output information has been written to 'output_info.txt'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'tag'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tag'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Assuming you have your data and labels\u001b[39;00m\n\u001b[0;32m      9\u001b[0m X \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestionText\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m---> 10\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtag\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Tokenize and preprocess text data\u001b[39;00m\n\u001b[0;32m     13\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tag'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Assuming you have your data and labels\n",
    "X = df['questionText'].values\n",
    "y = df['tag'].values\n",
    "\n",
    "# Tokenize and preprocess text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "X = pad_sequences(X)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_seq_length))\n",
    "model.add(LSTM(units=100))\n",
    "model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vocab_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Build the model\u001b[39;00m\n\u001b[0;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[1;32m---> 20\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Embedding(input_dim\u001b[38;5;241m=\u001b[39mvocab_size, output_dim\u001b[38;5;241m=\u001b[39membedding_dim, input_length\u001b[38;5;241m=\u001b[39mmax_seq_length))\n\u001b[0;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39madd(LSTM(units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m))\n\u001b[0;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(units\u001b[38;5;241m=\u001b[39mnum_classes, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab_size' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming you have your data and labels\n",
    "X = df['questionText'].values\n",
    "y = df['questionType'].values  # Replace 'tag' with the actual column name\n",
    "\n",
    "# Tokenize and preprocess text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "X = pad_sequences(X)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_seq_length))\n",
    "model.add(LSTM(units=100))\n",
    "model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
